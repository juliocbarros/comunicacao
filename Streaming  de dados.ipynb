{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcff5d3f-6439-467e-b826-1602edd21740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install faker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbfbf88-ac5a-4163-8561-f45645910caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import *\n",
    "import uuid\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Setup do Faker\n",
    "faker = Faker()\n",
    "\n",
    "# Caminho de saída para bronze (pode ser DBFS ou montado)\n",
    "caminho_bronze = \"/Volumes/comunicacao/comunicacao/bronze/stream_dados.parquet\"\n",
    "\n",
    "# Define o schema correspondente ao seu CSV\n",
    "schema = StructType([\n",
    "    StructField(\"Nome\", StringType(), True),\n",
    "    StructField(\"Endereço\", StringType(), True),\n",
    "    StructField(\"IP\", StringType(), True),\n",
    "    StructField(\"Hora da conexão\", StringType(), True),\n",
    "    StructField(\"Dispositivo de acesso\", StringType(), True),\n",
    "    StructField(\"Velocidade de conexão\", StringType(), True),\n",
    "    StructField(\"Status de conexão\", StringType(), True),\n",
    "    StructField(\"Rescued data\", StringType(), True),\n",
    "    StructField(\"Rastreamento\", StringType(), True),\n",
    "    StructField(\"Sources\", StringType(), True),\n",
    "    StructField(\"Fonte\", StringType(), True),\n",
    "    StructField(\"Data\", StringType(), True),\n",
    "    StructField(\"Arquivo em gestão\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Função para gerar um lote de dados (como Pandas DataFrame)\n",
    "def gerar_lote(qtd=10):\n",
    "    registros = []\n",
    "    for _ in range(qtd):\n",
    "        inicio = faker.date_time_between(start_date='-7d', end_date='now')\n",
    "        fim = inicio + timedelta(minutes=random.randint(10, 60))\n",
    "\n",
    "        registros.append({\n",
    "            \"Nome\": faker.name(),\n",
    "            \"Endereço\": faker.address().replace('\\n', ', '),\n",
    "            \"IP\": faker.ipv4(),\n",
    "            \"Hora da conexão\": f\"{inicio.strftime('%Y-%m-%d %H:%M:%S')} - {fim.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            \"Dispositivo de acesso\": random.choice([\"Celular\", \"Computador\", \"Tablet\"]),\n",
    "            \"Velocidade de conexão\": f\"{round(random.uniform(5.0, 500.0), 2)} Mbps\",\n",
    "            \"Status de conexão\": random.choice([\"Conectado\", \"Desconectado\"]),\n",
    "            \"Rescued data\": str(uuid.uuid4()),\n",
    "            \"Rastreamento\": str(uuid.uuid4()),\n",
    "            \"Sources\": random.choice([\"Web Gateway\", \"Firewall\", \"Router Log\"]),\n",
    "            \"Fonte\": faker.domain_name(),\n",
    "            \"Data\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"Arquivo em gestão\": random.choice([\"save.log\", \"top.log\", \"difference.log\"]),\n",
    "            \"Status\": random.choice([\"True\", \"False\"])\n",
    "        })\n",
    "    return registros\n",
    "\n",
    "# Cria um DataFrame estático e transforma em streaming simulado\n",
    "def gerar_streaming():\n",
    "    dados_iniciais = gerar_lote(100)\n",
    "    df_inicial = spark.createDataFrame(pd.DataFrame(dados_iniciais), schema=schema)\n",
    "    df_stream = df_inicial.writeStream \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"simulacao_memoria\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start()\n",
    "    return df_stream\n",
    "\n",
    "# Função para criar dados e salvar em parquet com foreachBatch\n",
    "def gerar_continuamente(intervalo_segundos=5):\n",
    "    from pyspark.sql.functions import lit\n",
    "    from pyspark.sql.streaming import DataStreamWriter\n",
    "\n",
    "    def gerar_microbatch(_):\n",
    "        dados = gerar_lote(10)\n",
    "        df_batch = spark.createDataFrame(pd.DataFrame(dados), schema=schema)\n",
    "        df_batch.write.mode(\"append\").parquet(caminho_bronze)\n",
    "\n",
    "    from threading import Timer\n",
    "    def ciclo():\n",
    "        gerar_microbatch(None)\n",
    "        Timer(intervalo_segundos, ciclo).start()\n",
    "\n",
    "    ciclo()\n",
    "\n",
    "# ---- EXECUÇÃO ----\n",
    "gerar_continuamente(intervalo_segundos=5)  # Gera dados a cada 5 segundos\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming  de dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
